{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"debugger/","title":"Policy Debugger","text":"<p>Step through trained policies to inspect actions, states, and Q-values. Uses Rich for clean console tables.</p>"},{"location":"debugger/#how-it-works","title":"How It Works","text":"<p>Load a model and simulate steps, highlighting decisions.</p>"},{"location":"debugger/#example-usage","title":"Example Usage","text":"<p>protokit debug --model tuned_model.pth --steps 10</p> <p>Output looks like: +------+---------+--------+-----------+ | Step | State | Action | Q-Values | +------+---------+--------+-----------+ | 0 | [0.02, | 1 | [0.1, 0.5]| | | 0.03...] | | | +------+---------+--------+-----------+</p> <p>!!! warning \"Limitations\"     Currently supports simple DQN; expand for PPO in <code>debugger.py</code>.</p> <p>Try the demo button to step through a virtual policy!</p>"},{"location":"full/","title":"Full Pipeline","text":"<p>Chain everything together for end-to-end prototyping: generate wrapper \u2192 tune params \u2192 debug policy.</p>"},{"location":"full/#example-usage","title":"Example Usage","text":"<p>protokit full --env CartPole-v1 --mods \"scale_rewards=0.5\" --params \"lr: [0.001, 0.01]\" --trials 5 --debug_steps 10</p> <p>text</p> <p>This automates the workflow in <code>protokit.py</code>, producing a tuned model and debug output.</p>"},{"location":"full/#why-its-useful","title":"Why It's Useful","text":"<ul> <li>Saves time on iterative RL experiments.</li> <li>Modular: Mix and match commands as needed.</li> </ul> <p>!!! success \"TE-Inspired Modularity\"     Like tweaking knobs on a synth, adjust one part without rebuilding the whole.</p> <p>Hit \"Run Demo\" to simulate the full flow! With these in place, run mkdocs serve again. The site now has navigable tabs, searchable content, and interactive elements (e.g., the floating pink \"Run Demo\" button that alerts a simulation message). The design stays minimalist and engaging, with fade-in animations and circular buttons evoking Teenage Engineering's hardware aesthetic.</p> <p>Installing and Testing the Tool Let's get it running and verified.</p> <p>Install the Package From inside RL_ProtoKit, run:</p> <p>bash pip install -e . This makes protokit available as a command.</p> <p>Run Unit Tests Ensure everything works with the tests we set up:</p> <p>bash pytest tests/ All should pass (they check basic functionality like wrapper generation and tuning output). If any fail, check imports or dependencies.</p> <p>Test Individual Commands</p> <p>Wrapper: protokit generate --env CartPole-v1 --mods \"scale_rewards=0.5\" \u2192 Check custom_wrapper.py is created and importable.</p> <p>Tuner: protokit tune --params \"lr: [0.001, 0.01]\" --trials 5 \u2192 It should train quickly and save tuned_model.pth.</p> <p>Debugger: protokit debug --model tuned_model.pth --steps 10 \u2192 Watch the Rich table in your console.</p> <p>Full Pipeline: protokit full --env CartPole-v1 --mods \"scale_rewards=0.5\" --params \"lr: [0.001, 0.01]\" --trials 5 --debug_steps 10 \u2192 Runs everything sequentially.</p> <p>Common Fixes</p> <p>If multiprocessing errors occur, ensure your system supports it (e.g., add if name == 'main': guards if needed).</p> <p>For real RL training, expand the train_trial loop in tuner.py with proper updates\u2014 the current version is a demo placeholder.</p>"},{"location":"home/","title":"Home - RL ProtoKit Overview","text":"<p>Welcome to RL ProtoKit! Explore the navigation for command details.</p>"},{"location":"tuner/","title":"Hyperparam Tuner","text":"<p>Run lightweight grid searches to optimize RL hyperparameters efficiently. It uses multiprocessing for speed on small setups.</p>"},{"location":"tuner/#key-features","title":"Key Features","text":"<ul> <li>Parallel trials for params like learning rate.</li> <li>Outputs the best model based on average rewards.</li> </ul>"},{"location":"tuner/#example-usage","title":"Example Usage","text":"<p>protokit tune --params \"lr: [0.001, 0.01]\" --trials 5</p> <p>text</p> <p>Behind the scenes, it trains a simple DQN: From tuner.py (simplified) def train_trial(params): lr, env_name = params env = gym.make(env_name) model = SimpleDQN(4, 2) # State and action sizes for CartPole</p>"},{"location":"tuner/#training-loop-here","title":"Training loop here...","text":"<p>return total_reward, lr</p> <p>text</p> <p>Results are aggregated in a Pandas DataFrame for easy ranking.</p> <p>!!! note \"Customization\"     Parse more params in <code>tuner.py</code> for gamma or batch size.</p> <p>Use the interactive button to mock a tuning run!</p>"},{"location":"wrapper/","title":"Wrapper Generator","text":"<p>This module generates custom Gym wrappers to modify environments quickly. It's perfect for tweaking rewards or states without manual coding.</p>"},{"location":"wrapper/#how-it-works","title":"How It Works","text":"<p>Specify a base environment and modifications via the CLI. The tool outputs a ready-to-use Python file.</p>"},{"location":"wrapper/#example-usage","title":"Example Usage","text":"<p>protokit generate --env CartPole-v1 --mods \"scale_rewards=0.5\"</p> <p>text</p> <p>This creates <code>custom_wrapper.py</code> with code like: import gymnasium as gym</p> <p>class CustomWrapper(gym.Wrapper): def init(self, env): super().init(env)</p> <p>text def step(self, action):     obs, reward, terminated, truncated, info = self.env.step(action)     reward *= 0.5  # Applied scaling     return obs, reward, terminated, truncated, info Load it: env = CustomWrapper(gym.make('CartPole-v1')) text</p> <p>!!! tip \"Pro Tip\"     Extend mods by editing <code>wrapper_generator.py</code>\u2014add noise or clipping for more flair.</p> <p>Click the \"Run Demo\" button on the site to simulate a wrapped env step!</p>"}]}